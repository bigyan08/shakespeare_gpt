{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6755d673",
      "metadata": {
        "id": "6755d673"
      },
      "source": [
        "## Imports and Device Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "dc2b4863",
      "metadata": {
        "id": "dc2b4863"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "3a3fdd61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3a3fdd61",
        "outputId": "2e79aab5-6ea6-4e8e-8df7-8833a4371cd3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.8.0+cu126'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e8dcbf23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e8dcbf23",
        "outputId": "fdad6b21-4ddb-4dde-eb71-03a429adfdda"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e6dcebe",
      "metadata": {
        "id": "1e6dcebe"
      },
      "source": [
        "## Getting the dataset ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "aba64529",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aba64529",
        "outputId": "b272806a-8047-4600-cac8-01f59488d875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-09-17 11:30:01--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2025-09-17 11:30:01 (143 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e9063316",
      "metadata": {
        "id": "e9063316"
      },
      "outputs": [],
      "source": [
        "with open('input.txt','r',encoding='utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6342e656",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6342e656",
        "outputId": "37549b7d-804c-4c73-810e-221d26d98069"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor\",\n",
              " 1115394)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[:500], len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "d91977c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d91977c8",
        "outputId": "3522bec7-de72-4e01-f7b5-763c10890faf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '$',\n",
              " '&',\n",
              " \"'\",\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '3',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z'}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "aeafd26d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeafd26d",
        "outputId": "607731b1-cb09-4118-9ccf-68eaed100cc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "chars = list(sorted(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "90d2751e",
      "metadata": {
        "id": "90d2751e"
      },
      "outputs": [],
      "source": [
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e9bf44",
      "metadata": {
        "id": "50e9bf44"
      },
      "source": [
        "### Causal Masking function:\n",
        "* This function creates a mask to prevent the model from attending to future tokens during training. It ensures that the prediction for a token only depends on the tokens that come before it in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "32038795",
      "metadata": {
        "id": "32038795"
      },
      "outputs": [],
      "source": [
        "def build_causal_mask(seq_len, device):\n",
        "    # Returns shape (1, 1, seq_len, seq_len)\n",
        "    mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool, device=device))\n",
        "    return mask.unsqueeze(0).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d12d57f",
      "metadata": {
        "id": "5d12d57f"
      },
      "source": [
        "## Token Embedding:\n",
        "Converting the tokens into vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d434d4e",
      "metadata": {
        "id": "2d434d4e"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self,vocab_size,d_model):\n",
        "        super(TokenEmbedding,self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=d_model)\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.embedding(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f09df1a2",
      "metadata": {
        "id": "f09df1a2"
      },
      "source": [
        "## Positional Encoding:\n",
        "* Adding positional encodings to the token embeddings to provide information about the position of each token in the sequence. This is crucial for the model to understand the order of tokens.\n",
        "* Here we use sinusoidal positional encoding as described in the \"Attention is All You Need\" paper. This works by using sine and cosine functions of different frequencies to generate unique positional encodings for each position in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "10a79b9d",
      "metadata": {
        "id": "10a79b9d"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  #(1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        returns the sum of embeddings and positional encoding, hence positional encoded embeddings\n",
        "        '''\n",
        "        # shape of x is now: (batch_size, seq_len, d_model)\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:, :seq_len, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e0c96a",
      "metadata": {
        "id": "86e0c96a"
      },
      "source": [
        "## Multi-Head Self-Attention:\n",
        "* This mechanism allows the model to focus on different parts of the input sequence when making predictions.\n",
        "* It computes attention scores using the query, key, and value matrices derived from the input embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "bfbe135d",
      "metadata": {
        "id": "bfbe135d"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, d_model):\n",
        "        super(MultiHeadAttention,self).__init__()\n",
        "        assert d_model % num_heads == 0   # d_model must be divisible by num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "\n",
        "        self.W_q = nn.Linear(d_model,d_model)\n",
        "        self.W_k = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model,d_model)\n",
        "        self.W_o = nn.Linear(d_model,d_model)\n",
        "\n",
        "    def forward(self,query,key,value,mask):\n",
        "        '''\n",
        "        shape of query,key,value : batch_size, seq_len, d_model\n",
        "        '''\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        Q = self.W_q(query)\n",
        "        K = self.W_k(key)\n",
        "        V = self.W_v(value)\n",
        "\n",
        "        Q = Q.view(batch_size,-1,self.num_heads,self.d_k).transpose(1,2)\n",
        "        K = K.view(batch_size,-1,self.num_heads,self.d_k).transpose(1,2)\n",
        "        V = V.view(batch_size,-1,self.num_heads,self.d_k).transpose(1,2)\n",
        "\n",
        "        scores = torch.matmul(Q,K.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
        "        # [B, H, L, d_k] × [B, H, d_k, L] → [B, H, L, L]\n",
        "\n",
        "        if mask is not None:\n",
        "            if mask.dim() == 4 and mask.size(-1) == scores.size(-1):\n",
        "                scores = scores.masked_fill(~mask, float('-inf'))\n",
        "            else:\n",
        "                raise ValueError(f\"Mask shape {mask.shape} not compatible with scores {scores.shape}\")\n",
        "\n",
        "\n",
        "        attn = torch.softmax(scores,dim=-1)\n",
        "        output = torch.matmul(attn,V)\n",
        "        #[B, H, L, L] × [B, H, L, d_k] → [B, H, L, d_k]\n",
        "\n",
        "        output = output.transpose(1,2).contiguous().view(batch_size,-1,self.d_model)\n",
        "\n",
        "        return self.W_o(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc5fe39",
      "metadata": {
        "id": "dcc5fe39"
      },
      "source": [
        "## Feed-Forward Neural Network:\n",
        "* A simple two-layer feed-forward neural network with a ReLU activation function in between. This helps in learning complex patterns in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "9014df4c",
      "metadata": {
        "id": "9014df4c"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model,hidden_layer,dropout=0.1):\n",
        "        super(FeedForward,self).__init__()\n",
        "        self.layer1 = nn.Linear(d_model,hidden_layer)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_layer,d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.layer2(self.dropout(self.relu(self.layer1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "735f3c08",
      "metadata": {
        "id": "735f3c08"
      },
      "source": [
        "## Normalization and Residual Connections:\n",
        "* Layer normalization is applied to stabilize and accelerate training.\n",
        "* Residual connections help in mitigating the vanishing gradient problem and allow for deeper networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ba84cdf2",
      "metadata": {
        "id": "ba84cdf2"
      },
      "outputs": [],
      "source": [
        "class NormResidual(nn.Module):\n",
        "    def __init__(self,d_model,dropout=0.1):\n",
        "        super(NormResidual,self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,x, sublayer_output):\n",
        "        return self.layernorm(x + self.dropout(sublayer_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a247c61",
      "metadata": {
        "id": "5a247c61"
      },
      "source": [
        "## Decoder Layer:\n",
        "* Combines the multi-head self-attention and feed-forward neural network with normalization and residual connections.\n",
        "* This is a decoder-based architecture, so it does not include the encoder part. The decoder is designed to generate text based on the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "991b41bb",
      "metadata": {
        "id": "991b41bb"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self,d_model,num_heads,hidden_layer,dropout=0.1):\n",
        "        super(DecoderLayer,self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(num_heads,d_model)\n",
        "        self.res_output1 = NormResidual(d_model,dropout)\n",
        "        self.ff_output = FeedForward(d_model,hidden_layer,dropout)\n",
        "        self.res_output2 = NormResidual(d_model,dropout)\n",
        "\n",
        "    def forward(self,x,mask):\n",
        "        x = self.res_output1(x, self.self_attn(x,x,x,mask))\n",
        "        x = self.res_output2(x,self.ff_output(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7f32997",
      "metadata": {
        "id": "c7f32997"
      },
      "source": [
        "## Transformer Model:\n",
        "* Stacks multiple decoder layers to form the complete transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a73d3e4",
      "metadata": {
        "id": "2a73d3e4"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self,vocab_size,d_model,num_heads,hidden_layer,num_dec,max_len,dropout):\n",
        "        super(Transformer,self).__init__()\n",
        "        self.token_emb = TokenEmbedding(vocab_size,d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model,max_len)\n",
        "        self.decoder = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, hidden_layer, dropout)\n",
        "            for _ in range(num_dec)\n",
        "        ])\n",
        "        self.output_layer = nn.Linear(d_model,vocab_size)\n",
        "\n",
        "    def forward(self,idx):\n",
        "        B,L = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        x = self.pos_enc(x)\n",
        "        mask = build_causal_mask(L,device)\n",
        "        for layer in self.decoder:\n",
        "            x = layer(x,mask)\n",
        "        output = self.output_layer(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "615785fd",
      "metadata": {
        "id": "615785fd"
      },
      "source": [
        "## Hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "e3714420",
      "metadata": {
        "id": "e3714420"
      },
      "outputs": [],
      "source": [
        "d_model=512\n",
        "num_heads=4\n",
        "hidden_layer=512\n",
        "num_dec=4\n",
        "block_size=128\n",
        "batch_size=64\n",
        "dropout=0.1\n",
        "epochs=3\n",
        "learning_rate=3e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4418760d",
      "metadata": {
        "id": "4418760d"
      },
      "source": [
        "## Train Test Split and Preparation of DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "85854453",
      "metadata": {
        "id": "85854453"
      },
      "outputs": [],
      "source": [
        "data = [stoi[token] for token in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "84b16aee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84b16aee",
        "outputId": "021e61cb-9a81-474f-9411-be21e0eb07a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1003854, 111540, 1115394, 1115394)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n = int(0.9*len(data))\n",
        "train = data[:n]\n",
        "test = data[n:]\n",
        "len(train),len(test),len(train) + len(test), len(data) # 90% train data, 10% test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "5b8a2ec5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b8a2ec5",
        "outputId": "4ef98406-ef20-4a32-cb8f-fbddd51ee00b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7b5054975e50>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7b51529ec560>)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_seqs = []\n",
        "target_seqs = []\n",
        "input_seqs_test = []\n",
        "target_seqs_test = []\n",
        "for i in range(0,len(train)-block_size):\n",
        "    input_seqs.append(train[i:i+block_size])\n",
        "    target_seqs.append(train[i+1:i+block_size+1])\n",
        "\n",
        "for i in range(0,len(test)-block_size):\n",
        "    input_seqs_test.append(test[i:i+block_size])\n",
        "    target_seqs_test.append(test[i+1:i+block_size+1])\n",
        "\n",
        "input_tensor = torch.tensor(input_seqs,dtype=torch.long)\n",
        "target_tensor = torch.tensor(target_seqs,dtype=torch.long)\n",
        "input_tensor_test = torch.tensor(input_seqs_test,dtype=torch.long)\n",
        "target_tensor_test = torch.tensor(target_seqs_test,dtype=torch.long)\n",
        "\n",
        "dataset_train = TensorDataset(input_tensor,target_tensor)\n",
        "dataset_test = TensorDataset(input_tensor_test,target_tensor_test)\n",
        "train_loader = DataLoader(dataset_train,batch_size,shuffle=True)\n",
        "test_loader = DataLoader(dataset_test,batch_size,shuffle=True)\n",
        "\n",
        "train_loader,test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84696e28",
      "metadata": {
        "id": "84696e28"
      },
      "source": [
        "## Training:\n",
        "* The model is trained using the Adam optimizer and cross-entropy loss function.\n",
        "* The training loop iterates over the dataset for a specified number of epochs, updating the model weights based on the computed loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "131e53e4",
      "metadata": {
        "id": "131e53e4"
      },
      "outputs": [],
      "source": [
        "model = Transformer(vocab_size, d_model=d_model, num_heads=num_heads, hidden_layer=hidden_layer, num_dec=num_dec, max_len=block_size, dropout=dropout).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "3b36f75e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b36f75e",
        "outputId": "4233ac39-7175-4c79-e012-e0b66570f0cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [1/3]: 100%|██████████| 15684/15684 [33:03<00:00,  7.91it/s, avg_loss=1.12, batch_loss=0.872]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 complete. Average Train Loss: 1.1190\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [2/3]: 100%|██████████| 15684/15684 [32:54<00:00,  7.94it/s, avg_loss=0.752, batch_loss=0.627]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 complete. Average Train Loss: 0.7519\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [3/3]: 100%|██████████| 15684/15684 [32:50<00:00,  7.96it/s, avg_loss=0.624, batch_loss=0.568]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 complete. Average Train Loss: 0.6244\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(loop):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)  # (batch, seq, vocab_size)\n",
        "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "        loop.set_postfix(batch_loss=loss.item(),\n",
        "                         avg_loss=total_train_loss/(batch_idx+1))\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} complete. Average Train Loss: {avg_train_loss:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adc4b414",
      "metadata": {
        "id": "adc4b414"
      },
      "source": [
        "## Saving the model:\n",
        "* The trained model is saved in a file named 'mini_gpt.pth'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "3696a60b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3696a60b",
        "outputId": "7212bef7-d3aa-4ecb-ab4c-b16cdc2daf42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.save(model.state_dict(),'mini_gpt.pth')\n",
        "model = Transformer(vocab_size, d_model=d_model, num_heads=num_heads, hidden_layer=hidden_layer, num_dec=num_dec, max_len=block_size, dropout=dropout).to(device)\n",
        "model.load_state_dict(torch.load('mini_gpt.pth',map_location=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba8a13e2",
      "metadata": {
        "id": "ba8a13e2"
      },
      "source": [
        "## Text Generation:\n",
        "* A function to generate text based on a given prompt using the trained model. It uses temperature sampling to introduce randomness in the generated text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "f4765ef0",
      "metadata": {
        "id": "f4765ef0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_text(model, prompt, stoi, itos, num_generate,temperature):\n",
        "    '''\n",
        "    Model: Transformer Model\n",
        "    prompt: Some text to start with.\n",
        "    stoi,itos: mapping dictionaries.\n",
        "    block_size: maximum context length for the model (use the same value which the model was trained on.)\n",
        "    '''\n",
        "    input_indices = [stoi[ch] for ch in prompt]\n",
        "    generated = input_indices.copy()\n",
        "    model.eval()\n",
        "    for _ in range(num_generate):\n",
        "        # Truncate to block_size\n",
        "        input_seq = generated[-block_size:] if len(generated) > block_size else generated\n",
        "        x = torch.tensor([input_seq], dtype=torch.long).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)  # (1, seq_len, vocab_size)\n",
        "            logits = logits[0, -1, :] / temperature  # last position\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            next_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "        generated.append(next_idx)\n",
        "    return ''.join([itos[i] for i in generated])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "1593577f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1593577f",
        "outputId": "59b41341-8b1a-4963-d94c-37439570ce9e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Romeo:\\nYour highness are ready, stumbled at that measure\\nWhich know the poor will I wish you: if you will,'"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model,'Romeo:',stoi,itos,100,1.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e2e64f2",
      "metadata": {
        "id": "6e2e64f2"
      },
      "source": [
        "## Evaluation:\n",
        "* The model is evaluated on a validation set to monitor its performance and prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "d4b88e72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4b88e72",
        "outputId": "8a902312-7b16-46c9-d1d1-2a37533307ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 complete. Average Test Loss: 2.4363\n",
            "Epoch 2 complete. Average Test Loss: 2.4362\n",
            "Epoch 3 complete. Average Test Loss: 2.4363\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "  model.eval()\n",
        "  total_test_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (x, y) in enumerate(test_loader):\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      logits = model(x)  # (batch, seq, vocab_size)\n",
        "      test_loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
        "      total_test_loss += test_loss.item()\n",
        "  avg_test_loss = total_test_loss / len(test_loader)\n",
        "  print(f\"Epoch {epoch+1} complete. Average Test Loss: {avg_test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I_Qap9mGiGWm",
      "metadata": {
        "id": "I_Qap9mGiGWm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
